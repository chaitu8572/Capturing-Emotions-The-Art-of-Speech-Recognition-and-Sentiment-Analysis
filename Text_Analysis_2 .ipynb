{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/vnatireddi1/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set class distribution:\n",
      "neutral     4710\n",
      "joy         1743\n",
      "surprise    1205\n",
      "anger       1109\n",
      "sadness      683\n",
      "disgust      271\n",
      "fear         268\n",
      "Name: Emotion, dtype: int64\n",
      "neutral     4710\n",
      "negative    2945\n",
      "positive    2334\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Print the class distribution for the training set\n",
    "import pandas as pd\n",
    "data = pd.read_csv('/home/vnatireddi1/projects/meld/Text_Analysis/train_sent_emo.csv', encoding='latin1')\n",
    "print(\"Training set class distribution:\")\n",
    "print(data['Emotion'].value_counts())\n",
    "print(data['Sentiment'].value_counts())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: imbalanced-learn in /home/vnatireddi1/.local/lib/python3.8/site-packages (0.10.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/vnatireddi1/.local/lib/python3.8/site-packages (from imbalanced-learn) (1.23.5)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/vnatireddi1/.local/lib/python3.8/site-packages (from imbalanced-learn) (3.1.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /home/vnatireddi1/.local/lib/python3.8/site-packages (from imbalanced-learn) (1.2.0)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /home/vnatireddi1/.local/lib/python3.8/site-packages (from imbalanced-learn) (1.10.1)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /home/vnatireddi1/.local/lib/python3.8/site-packages (from imbalanced-learn) (1.2.2)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.8 -m pip install --upgrade pip\u001b[0m\n",
      "Original training set class distribution:\n",
      "neutral     4710\n",
      "negative    2945\n",
      "positive    2334\n",
      "Name: Sentiment, dtype: int64\n",
      "\n",
      "Resampled training set class distribution:\n",
      "negative    2945\n",
      "neutral     2334\n",
      "positive    2334\n",
      "Name: Sentiment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#!pip install imbalanced-learn\n",
    "\n",
    "import pandas as pd\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Load the train dataset\n",
    "train_df = pd.read_csv('/home/vnatireddi1/projects/meld/Text_Analysis/train_sent_emo.csv', encoding='latin1')\n",
    "\n",
    "# Print the class distribution for the original training set\n",
    "print(\"Original training set class distribution:\")\n",
    "print(train_df['Sentiment'].value_counts())\n",
    "\n",
    "# Split the training set into features and target label\n",
    "X_train = train_df['Utterance'].values\n",
    "y_train = train_df['Sentiment'].values\n",
    "\n",
    "# Undersample the majority class (neutral) using RandomUnderSampler\n",
    "undersampler = RandomUnderSampler(sampling_strategy={'neutral': int(train_df['Sentiment'].value_counts().min())}, random_state=42)\n",
    "X_train_resampled, y_train_resampled = undersampler.fit_resample(X_train.reshape(-1, 1), y_train)\n",
    "\n",
    "# Convert the undersampled data back to pandas dataframe\n",
    "train_resampled_df = pd.DataFrame({'Utterance': X_train_resampled.squeeze(), 'Sentiment': y_train_resampled})\n",
    "\n",
    "# Print the class distribution for the resampled training set\n",
    "print(\"\\nResampled training set class distribution:\")\n",
    "print(train_resampled_df['Sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vnatireddi1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text) # remove URLs\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove special characters\n",
    "    text = text.lower() # convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    text = ' '.join(filtered_tokens)\n",
    "    return text\n",
    "\n",
    "train_resampled_df['clean_text'] = train_resampled_df['Utterance'].apply(lambda x: clean_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "train_resampled_df['Sentiment'] = le.fit_transform(train_resampled_df['Sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Utterance</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No donÂt I beg of you!</td>\n",
       "      <td>0</td>\n",
       "      <td>donât beg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>But then who? The waitress I went out with las...</td>\n",
       "      <td>0</td>\n",
       "      <td>waitress went last month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>You know? Forget it!</td>\n",
       "      <td>0</td>\n",
       "      <td>know forget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>No-no-no-no, no! Who, who were you talking about?</td>\n",
       "      <td>0</td>\n",
       "      <td>nononono talking</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>No, I-I-I-I don't, I actually don't know</td>\n",
       "      <td>0</td>\n",
       "      <td>iiii dont actually dont know</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Utterance  Sentiment  \\\n",
       "0                            No donÂt I beg of you!          0   \n",
       "1  But then who? The waitress I went out with las...          0   \n",
       "2                               You know? Forget it!          0   \n",
       "3  No-no-no-no, no! Who, who were you talking about?          0   \n",
       "4           No, I-I-I-I don't, I actually don't know          0   \n",
       "\n",
       "                     clean_text  \n",
       "0                     donât beg  \n",
       "1      waitress went last month  \n",
       "2                   know forget  \n",
       "3              nononono talking  \n",
       "4  iiii dont actually dont know  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_resampled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train_resampled_df['clean_text'], train_resampled_df['Sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply count vectorization and tf-idf transformation to the training and testing data\n",
    "cv = CountVectorizer()\n",
    "X_train_counts = cv.fit_transform(X_train)\n",
    "tfidf = TfidfTransformer()\n",
    "X_train_tfidf = tfidf.fit_transform(X_train_counts)\n",
    "X_test_counts = cv.transform(X_test)\n",
    "X_test_tfidf = tfidf.transform(X_test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.520682862770847\n",
      "Classification report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.55      0.66      0.60       617\n",
      "     neutral       0.46      0.39      0.42       454\n",
      "    positive       0.53      0.46      0.49       452\n",
      "\n",
      "    accuracy                           0.52      1523\n",
      "   macro avg       0.51      0.50      0.50      1523\n",
      "weighted avg       0.52      0.52      0.51      1523\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vnatireddi1/.local/lib/python3.8/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# Train a logistic regression model on the preprocessed data\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Evaluate the logistic regression model on the testing data\n",
    "lr_pred = lr.predict(X_test_tfidf)\n",
    "accuracy = accuracy_score(y_test, lr_pred)\n",
    "report = classification_report(y_test, lr_pred, target_names=le.classes_)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification report:\\n\", report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MELD dataset\n",
    "with open('/home/vnatireddi1/projects/meld/Text_Analysis/train_sent_emo.csv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "for line in lines[1:]:\n",
    "    cols = line.strip().split(',')\n",
    "    text = cols[1].strip()\n",
    "    label = cols[2].strip()\n",
    "    X_train.append(text)\n",
    "    y_train.append(label)\n",
    "\n",
    "# with open('meld/dev_sent_emo.csv', 'r') as f:\n",
    "#     lines = f.readlines()\n",
    "\n",
    "# X_test = []\n",
    "# y_test = []\n",
    "# for line in lines[1:]:\n",
    "#     cols = line.strip().split(',')\n",
    "#     text = cols[1].strip()\n",
    "#     label = cols[2].strip()\n",
    "#     X_test.append(text)\n",
    "#     y_test.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x93 in position 6030: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-0a3e11d28048>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mglove\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove.6B.50d.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib64/python3.8/codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, input, final)\u001b[0m\n\u001b[1;32m    320\u001b[0m         \u001b[0;31m# decode input (taking the buffer into account)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 322\u001b[0;31m         \u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    323\u001b[0m         \u001b[0;31m# keep undecoded input until the next call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x93 in position 6030: invalid start byte"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the GloVe embeddings\n",
    "glove = {}\n",
    "with open('glove.6B.50d.txt', 'r', encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype='float32')\n",
    "        glove[word] = vector\n",
    "\n",
    "# Vectorize the text using GloVe embeddings\n",
    "X_train_vectors = []\n",
    "for text in X_train:\n",
    "    vectors = []\n",
    "    for word in nltk.word_tokenize(text.lower()):\n",
    "        if word in glove:\n",
    "            vectors.append(glove[word])\n",
    "    if not vectors:\n",
    "        vectors.append([0] * 300)\n",
    "    vector = np.mean(vectors, axis=0)\n",
    "    X_train_vectors.append(vector)\n",
    "\n",
    "X_test_vectors = []\n",
    "for text in X_test:\n",
    "    vectors = []\n",
    "    for word in nltk.word_tokenize(text.lower()):\n",
    "        if word in glove:\n",
    "            vectors.append(glove[word])\n",
    "    if not vectors:\n",
    "        vectors.append([0] * 300)\n",
    "    vector = np.mean(vectors, axis=0)\n",
    "    X_test_vectors.append(vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the classifier\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train_vectors, y_train)\n",
    "\n",
    "# Test the classifier\n",
    "y_pred = clf.predict(X_test_vectors)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM ON TEXT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the MLED dataset\n",
    "df = pd.read_csv('/home/vnatireddi1/projects/meld/Text_Analysis/train_sent_emo.csv', encoding='latin1')\n",
    "\n",
    "# Preprocess the dataset\n",
    "df = df[['Utterance', 'Sentiment']]  # Keep only the text and label columns\n",
    "df['Utterance'] = df['Utterance'].apply(lambda x: x.lower())  # Convert text to lowercase\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/vnatireddi1/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text) # remove URLs\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove special characters\n",
    "    text = text.lower() # convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    text = ' '.join(filtered_tokens)\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['Utterance'].apply(lambda x: clean_text(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define the maximum number of words to keep\n",
    "max_words = 10000\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['clean_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_text'])\n",
    "\n",
    "# Define the maximum sequence length\n",
    "max_sequence_length = 100\n",
    "\n",
    "# Pad the sequences to make them of equal length\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "labels = df['Sentiment'].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.8/site-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-c4cd1f2bce5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib64/python3.8/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.8/site-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/site-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/site-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/site-packages/keras/losses.py\", line 1789, in categorical_crossentropy\n        return backend.categorical_crossentropy(\n    File \"/usr/local/lib/python3.8/site-packages/keras/backend.py\", line 5083, in categorical_crossentropy\n        target.shape.assert_is_compatible_with(output.shape)\n\n    ValueError: Shapes (None, 1) and (None, 3) are incompatible\n"
     ]
    }
   ],
   "source": [
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(data))\n",
    "train_data = data[:train_size]\n",
    "\n",
    "train_data = np.array(train_data).astype('float32')\n",
    "\n",
    "train_labels = labels[:train_size]\n",
    "\n",
    "test_data = data[train_size:]\n",
    "test_data = np.array(test_data).astype('float32')\n",
    "\n",
    "test_labels = labels[train_size:]\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, train_labels, epochs=5, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "print('Test loss:', loss)\n",
    "print('Test accuracy:', accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "200/200 [==============================] - 42s 185ms/step - loss: 1.0275 - accuracy: 0.4928 - val_loss: 1.0012 - val_accuracy: 0.5022\n",
      "Epoch 2/5\n",
      "200/200 [==============================] - 38s 189ms/step - loss: 0.8500 - accuracy: 0.6147 - val_loss: 1.0400 - val_accuracy: 0.4903\n",
      "Epoch 3/5\n",
      "200/200 [==============================] - 37s 182ms/step - loss: 0.6956 - accuracy: 0.6959 - val_loss: 1.1391 - val_accuracy: 0.4847\n",
      "Epoch 4/5\n",
      "200/200 [==============================] - 31s 155ms/step - loss: 0.5844 - accuracy: 0.7497 - val_loss: 1.2428 - val_accuracy: 0.4978\n",
      "Epoch 5/5\n",
      "200/200 [==============================] - 36s 179ms/step - loss: 0.5193 - accuracy: 0.7719 - val_loss: 1.3396 - val_accuracy: 0.4916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f211434d220>"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the MLED dataset\n",
    "df = pd.read_csv('/home/vnatireddi1/projects/meld/Text_Analysis/train_sent_emo.csv', encoding='latin1')\n",
    "\n",
    "# Preprocess the dataset\n",
    "df = df[['Utterance', 'Sentiment']]  # Keep only the text and label columns\n",
    "df['Utterance'] = df['Utterance'].apply(lambda x: x.lower())  # Convert text to lowercase\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['Sentiment'] = le.fit_transform(df['Sentiment'])\n",
    "\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text) # remove URLs\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) # remove mentions\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # remove special characters\n",
    "    text = text.lower() # convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = text.split()\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    text = ' '.join(filtered_tokens)\n",
    "    return text\n",
    "\n",
    "df['clean_text'] = df['Utterance'].apply(lambda x: clean_text(x))\n",
    "\n",
    "\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Define the maximum number of words to keep\n",
    "max_words = 10000\n",
    "\n",
    "# Tokenize the text data\n",
    "tokenizer = Tokenizer(num_words=max_words)\n",
    "tokenizer.fit_on_texts(df['clean_text'])\n",
    "sequences = tokenizer.texts_to_sequences(df['clean_text'])\n",
    "\n",
    "# Define the maximum sequence length\n",
    "max_sequence_length = 100\n",
    "\n",
    "# Pad the sequences to make them of equal length\n",
    "data = pad_sequences(sequences, maxlen=max_sequence_length)\n",
    "labels = df['Sentiment'].values\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Embedding, Dense\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_words, 128))\n",
    "model.add(LSTM(128, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(0.8 * len(data))\n",
    "train_data = data[:train_size]\n",
    "train_labels = labels[:train_size]\n",
    "test_data = data[train_size:]\n",
    "test_labels = labels[train_size:]\n",
    "\n",
    "# Convert the labels to one-hot encoded vectors/\n",
    "train_labels = to_categorical(train_labels, num_classes=3)\n",
    "test_labels = to_categorical(test_labels, num_classes=3)\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, train_labels, epochs=5, batch_size=32, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 - 1s - loss: 1.3239 - accuracy: 0.5030 - 1s/epoch - 23ms/step\n",
      "Test accuracy: 0.5030030012130737\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Evaluate the model on test data\n",
    "test_loss, test_acc = model.evaluate(test_data, test_labels, verbose=2)\n",
    "\n",
    "# Predict the test labels\n",
    "y_pred = model.predict(test_data)\n",
    "\n",
    "# Convert the predictions to class labels\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "test_labels = np.argmax(test_labels, axis=1)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Test accuracy:', test_acc)\n",
    "\n",
    "# Print the confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(test_labels, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: tensorflow in /usr/local/lib64/python3.8/site-packages (2.8.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.8/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.8/site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.8/site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.8/site-packages (from tensorflow) (13.0.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.8/site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/site-packages (from tensorflow) (51.1.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib64/python3.8/site-packages (from tensorflow) (1.14.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib64/python3.8/site-packages (from tensorflow) (3.6.0)\n",
      "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.8/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.8/site-packages (from tensorflow) (2.8.0.dev2021122109)\n",
      "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.8/site-packages (from tensorflow) (1.0.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.8/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.8/site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.8/site-packages (from tensorflow) (0.5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.8/site-packages (from tensorflow) (4.1.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib64/python3.8/site-packages (from tensorflow) (3.19.4)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib64/python3.8/site-packages (from tensorflow) (0.24.0)\n",
      "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.8/site-packages (from tensorflow) (2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/vnatireddi1/.local/lib/python3.8/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.8/site-packages (from tensorflow) (2.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib64/python3.8/site-packages (from tensorflow) (1.45.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.6.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.25.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.8/site-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.0.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (5.0.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.7.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (4.0.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2020.12.5)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.26.2)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.1.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython3.8 -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
